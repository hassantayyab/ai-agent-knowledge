# Evals 101: measuring agent quality without vibes (Chapter 25)

## Summary

- **Evals** turn agent quality into measurable signals (correctness, consistency, tool use, policy).
- A practical eval setup includes:
  - dataset of real tasks
  - expected outputs (or rubrics)
  - automated scoring where possible
  - human review for ambiguous cases
- The fastest evals come from mining **real traces**.

## Core concepts

### Types of evals

- **Unit**: narrow behavior (schema compliance, tool choice)
- **End-to-end**: full agent run (quality + cost + latency)
- **Regression**: rerun after changes to catch breakage

### Scoring approaches

- structured checks (schema, exact match, tolerances)
- heuristics (required sections, citations)
- LLM-as-judge with tight rubric (versioned)

## Practical takeaways

- Start with 20â€“50 examples and grow.
- Prefer structured scoring first; use judges only when needed.
- Gate prompt/tool/routing changes with regression evals.
